# Ability Card: Core

**File:** [`core.py`](file:///C:\Source-Codebase\research_review\pending\open-interpreter-main\interpreter\core\core.py)  
**Full Path:** `C:\Source-Codebase\research_review\pending\open-interpreter-main\interpreter\core\core.py`  
**Language:** Python  
**Analysis Level:** Basic

## Description

Python module with 16 imports detected.

## Technical Details

- **Functions:** 12
- **Classes:** 1
- **Imports:** 16
- **Complexity:** medium


## Functions

- **__init__**(self, messages, offline, auto_run, verbose, debug, max_output, safe_mode, shrink_images, loop, loop_message, loop_breakers, disable_telemetry, in_terminal_interface, conversation_history, conversation_filename, conversation_history_path, os, speak_messages, llm, system_message, custom_instructions, user_message_template, always_apply_user_message_template, code_output_template, empty_code_output_template, code_output_sender, computer, sync_computer, import_computer_api, skills_path, import_skills, multi_line, contribute_conversation, plain_text_display): No description available
- **local_setup**(self): Opens a wizard that lets terminal users pick a local model.
- **wait**(self): No description available
- **anonymous_telemetry**(self): No description available
- **will_contribute**(self): No description available
- **chat**(self, message, display, stream, blocking): No description available
- **_streaming_chat**(self, message, display): No description available
- **_respond_and_store**(self): Pulls from the respond stream, adding delimiters. Some things, like active_line, console, confirmation... these act specially.
- **reset**(self): No description available
- **display_message**(self, markdown): No description available
- **get_oi_dir**(self): No description available
- **is_ephemeral**(chunk): Ephemeral = this chunk doesn't contribute to a message we want to save.


## Classes

### OpenInterpreter

This class (one instance is called an `interpreter`) is the "grand central station" of this project.

Its responsibilities are to:

1. Given some user input, prompt the language model.
2. Parse the language models responses, converting them into LMC Messages.
3. Send code to the computer.
4. Parse the computer's response (which will already be LMC Messages).
5. Send the computer's response back to the language model.
...

The above process should repeat—going back and forth between the language model and the computer— until:

6. Decide when the process is finished based on the language model's response.

**Methods:**
- `__init__(self, messages, offline, auto_run, verbose, debug, max_output, safe_mode, shrink_images, loop, loop_message, loop_breakers, disable_telemetry, in_terminal_interface, conversation_history, conversation_filename, conversation_history_path, os, speak_messages, llm, system_message, custom_instructions, user_message_template, always_apply_user_message_template, code_output_template, empty_code_output_template, code_output_sender, computer, sync_computer, import_computer_api, skills_path, import_skills, multi_line, contribute_conversation, plain_text_display)`
- `local_setup(self)`: Opens a wizard that lets terminal users pick a local model
- `wait(self)`
- `anonymous_telemetry(self)`
- `will_contribute(self)`
- `chat(self, message, display, stream, blocking)`
- `_streaming_chat(self, message, display)`
- `_respond_and_store(self)`: Pulls from the respond stream, adding delimiters
- `reset(self)`
- `display_message(self, markdown)`
- `get_oi_dir(self)`

---
*Generated by AIPass-Code-Sniffer Basic Analyzer*
*Upgrade to enhanced/premium analysis for AI-powered insights*
